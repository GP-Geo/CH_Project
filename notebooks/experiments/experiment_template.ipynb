{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Channel Head Coupling Analysis - Experiment Template\n",
    "\n",
    "This notebook provides a clean template for running analysis experiments with different parameters.\n",
    "\n",
    "**Key features:**\n",
    "- All experiment parameters defined in one cell\n",
    "- Outputs automatically organized by experiment name/threshold\n",
    "- Easy to duplicate for new experiments\n",
    "\n",
    "## How to use:\n",
    "1. **Duplicate this notebook** for each experiment (e.g., `experiment_th500.ipynb`)\n",
    "2. **Modify the parameters** in the \"Experiment Configuration\" cell below\n",
    "3. **Run all cells** to execute the analysis\n",
    "4. Results are saved to `data/outputs/experiments/{EXPERIMENT_NAME}/`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment Configuration\n",
    "\n",
    "**Modify these parameters for your experiment:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXPERIMENT PARAMETERS - MODIFY THESE FOR YOUR EXPERIMENT\n",
    "# ============================================================================\n",
    "\n",
    "# Experiment identification\n",
    "EXPERIMENT_NAME = \"th500_test\"  # Unique name for this experiment\n",
    "EXPERIMENT_NOTES = \"Testing higher stream threshold (500 vs default 145)\"\n",
    "\n",
    "# Stream extraction threshold (default in paper: 145)\n",
    "# Higher = fewer, larger streams; Lower = more, smaller streams\n",
    "STREAM_THRESHOLD = 500\n",
    "\n",
    "# Basins to analyze (set to None for all 18 basins)\n",
    "# Examples:\n",
    "#   BASINS_TO_RUN = None  # All 18 basins\n",
    "#   BASINS_TO_RUN = [\"inyo\", \"humboldt\"]  # Just these two\n",
    "#   BASINS_TO_RUN = [\"inyo\"]  # Single basin for quick test\n",
    "BASINS_TO_RUN = [\"inyo\", \"humboldt\", \"calnalpine\"]  # Quick test set\n",
    "\n",
    "# Limit outlets per basin (None = all outlets, useful for quick tests)\n",
    "MAX_OUTLETS_PER_BASIN = None\n",
    "\n",
    "# Connectivity for coupling detection (4 or 8)\n",
    "# 8 = include diagonal neighbors (more sensitive)\n",
    "# 4 = only cardinal directions (more conservative)\n",
    "CONNECTIVITY = 8\n",
    "\n",
    "# ============================================================================\n",
    "print(f\"Experiment: {EXPERIMENT_NAME}\")\n",
    "print(f\"Notes: {EXPERIMENT_NOTES}\")\n",
    "print(f\"Stream threshold: {STREAM_THRESHOLD}\")\n",
    "print(f\"Basins: {BASINS_TO_RUN or 'All 18'}\")\n",
    "print(f\"Max outlets per basin: {MAX_OUTLETS_PER_BASIN or 'All'}\")\n",
    "print(f\"Connectivity: {CONNECTIVITY}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pathlib\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import topotoolbox as tt3\n",
    "\n",
    "# Add project root so \"channel_heads\" package is visible\n",
    "project_root = pathlib.Path(\"/Users/guypi/Projects/channel-heads\")\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Core analysis imports\n",
    "from channel_heads import (\n",
    "    CouplingAnalyzer,\n",
    "    first_meet_pairs_for_outlet,\n",
    "    outlet_node_ids_from_streampoi,\n",
    "    # Lengthwise asymmetry\n",
    "    LengthwiseAsymmetryAnalyzer,\n",
    "    compute_asymmetry_statistics,\n",
    "    merge_coupling_and_asymmetry,\n",
    "    # Basin configuration\n",
    "    get_z_th,\n",
    "    get_basin_config,\n",
    "    get_reference_delta_L,\n",
    "    BASIN_CONFIG,\n",
    "    # Path management\n",
    "    CROPPED_DEMS_DIR,\n",
    "    get_experiment_output_dir,\n",
    ")\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 120)\n",
    "%matplotlib inline\n",
    "\n",
    "# Create output directory for this experiment\n",
    "OUTPUT_DIR = get_experiment_output_dir(EXPERIMENT_NAME)\n",
    "print(f\"\\nOutput directory: {OUTPUT_DIR}\")\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basins-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Basin Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basins",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All 18 basins from Goren & Shelef (2024)\n",
    "ALL_BASINS = {\n",
    "    \"inyo\": {\"dem_path\": CROPPED_DEMS_DIR / \"Inyo_strm_crop.tif\", \"paper_name\": \"inyo\"},\n",
    "    \"humboldt\": {\"dem_path\": CROPPED_DEMS_DIR / \"Humboldt_strm_crop.tif\", \"paper_name\": \"humboldt\"},\n",
    "    \"calnalpine\": {\"dem_path\": CROPPED_DEMS_DIR / \"CalnAlpine_strm_crop.tif\", \"paper_name\": \"clanalpine\"},\n",
    "    \"daqing\": {\"dem_path\": CROPPED_DEMS_DIR / \"Daqing_strm_crop.tif\", \"paper_name\": \"daqing\"},\n",
    "    \"kammanasie\": {\"dem_path\": CROPPED_DEMS_DIR / \"Kammanasie_strm_crop.tif\", \"paper_name\": \"kammanassie\"},\n",
    "    \"luliang\": {\"dem_path\": CROPPED_DEMS_DIR / \"Luliang_strm_crop.tif\", \"paper_name\": \"luliang\"},\n",
    "    \"finisterre\": {\"dem_path\": CROPPED_DEMS_DIR / \"Finisterre_strm_crop.tif\", \"paper_name\": \"finisterre\"},\n",
    "    \"taiwan\": {\"dem_path\": CROPPED_DEMS_DIR / \"Taiwan_strm_crop.tif\", \"paper_name\": \"taiwan\"},\n",
    "    \"panamint\": {\"dem_path\": CROPPED_DEMS_DIR / \"Panamint_strm_crop.tif\", \"paper_name\": \"panamint\"},\n",
    "    \"sakhalin\": {\"dem_path\": CROPPED_DEMS_DIR / \"Sakhalin_strm_crop.tif\", \"paper_name\": \"sakhalin\"},\n",
    "    \"vallefertil\": {\"dem_path\": CROPPED_DEMS_DIR / \"ValleFertil_strm_crop.tif\", \"paper_name\": \"vallefertil\"},\n",
    "    \"sierramadre\": {\"dem_path\": CROPPED_DEMS_DIR / \"SierraMadre_strm_crop.tif\", \"paper_name\": \"sierramadre\"},\n",
    "    \"sierranevadaspain\": {\"dem_path\": CROPPED_DEMS_DIR / \"SierraNevadaSpain_strm_crop.tif\", \"paper_name\": \"sierranevada_spain\"},\n",
    "    \"piedepalo\": {\"dem_path\": CROPPED_DEMS_DIR / \"PieDePalo_strm_crop.tif\", \"paper_name\": \"piedepalo\"},\n",
    "    \"toano\": {\"dem_path\": CROPPED_DEMS_DIR / \"Toano_strm_crop.tif\", \"paper_name\": \"toano\"},\n",
    "    \"troodos\": {\"dem_path\": CROPPED_DEMS_DIR / \"Troodos_strm_crop.tif\", \"paper_name\": \"troodos\"},\n",
    "    \"tsugaru\": {\"dem_path\": CROPPED_DEMS_DIR / \"Tsugaru_strm_crop.tif\", \"paper_name\": \"tsugaru\"},\n",
    "    \"yoro\": {\"dem_path\": CROPPED_DEMS_DIR / \"Yoro_strm_crop.tif\", \"paper_name\": \"yoro\"},\n",
    "}\n",
    "\n",
    "# Filter to selected basins\n",
    "if BASINS_TO_RUN is not None:\n",
    "    BASINS_TO_ANALYZE = {k: v for k, v in ALL_BASINS.items() if k in BASINS_TO_RUN}\n",
    "else:\n",
    "    BASINS_TO_ANALYZE = ALL_BASINS\n",
    "\n",
    "# Show basin configurations\n",
    "print(\"=\" * 80)\n",
    "print(f\"Experiment: {EXPERIMENT_NAME}\")\n",
    "print(f\"Stream threshold: {STREAM_THRESHOLD}\")\n",
    "print(f\"Basins to analyze: {len(BASINS_TO_ANALYZE)}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for name, config in BASINS_TO_ANALYZE.items():\n",
    "    basin_config = get_basin_config(config[\"paper_name\"])\n",
    "    dem_exists = config['dem_path'].exists()\n",
    "    status = \"OK\" if dem_exists else \"MISSING\"\n",
    "    print(f\"  {basin_config['full_name'][:40]:<40} z_th={basin_config['z_th']:>4}m [{status}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functions-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_basin(basin_name, dem_path, z_th, stream_threshold):\n",
    "    \"\"\"Load DEM, apply z_th mask, and create stream network.\"\"\"\n",
    "    print(f\"Loading {basin_name}...\")\n",
    "    \n",
    "    # Load DEM\n",
    "    dem = tt3.read_tif(str(dem_path))\n",
    "    print(f\"  DEM shape: {dem.z.shape}\")\n",
    "    print(f\"  Elevation range: {np.nanmin(dem.z):.0f} - {np.nanmax(dem.z):.0f} m\")\n",
    "    \n",
    "    # Apply z_th mask\n",
    "    dem.z[dem.z < z_th] = np.nan\n",
    "    valid_pct = 100 * np.sum(~np.isnan(dem.z)) / dem.z.size\n",
    "    print(f\"  Applied z_th={z_th} m mask: {valid_pct:.1f}% valid pixels\")\n",
    "    \n",
    "    # Create flow and stream objects\n",
    "    fd = tt3.FlowObject(dem)\n",
    "    s = tt3.StreamObject(fd, threshold=stream_threshold)\n",
    "    \n",
    "    # Count network features\n",
    "    n_outlets = np.sum(s.streampoi('outlets'))\n",
    "    n_heads = np.sum(s.streampoi('channelheads'))\n",
    "    n_confluences = np.sum(s.streampoi('confluences'))\n",
    "    print(f\"  Stream network (threshold={stream_threshold}):\")\n",
    "    print(f\"    Outlets: {n_outlets}, Heads: {n_heads}, Confluences: {n_confluences}\")\n",
    "    \n",
    "    return {\"dem\": dem, \"fd\": fd, \"s\": s}\n",
    "\n",
    "\n",
    "def run_full_analysis(s, fd, dem, lat, connectivity, max_outlets=None):\n",
    "    \"\"\"Run coupling + lengthwise asymmetry analysis for all outlets.\"\"\"\n",
    "    outlets = outlet_node_ids_from_streampoi(s)\n",
    "    outlets = [int(o) for o in outlets]\n",
    "    \n",
    "    if max_outlets and len(outlets) > max_outlets:\n",
    "        print(f\"  Limiting to first {max_outlets} outlets (of {len(outlets)})\")\n",
    "        outlets = outlets[:max_outlets]\n",
    "    \n",
    "    # Create analyzers\n",
    "    coupling_an = CouplingAnalyzer(fd, s, dem, connectivity=connectivity)\n",
    "    asymmetry_an = LengthwiseAsymmetryAnalyzer(s, dem, lat=lat)\n",
    "    \n",
    "    coupling_dfs = []\n",
    "    asymmetry_dfs = []\n",
    "    \n",
    "    for idx, outlet in enumerate(outlets, 1):\n",
    "        print(f\"  [{idx}/{len(outlets)}] outlet={outlet}\", end=\"\", flush=True)\n",
    "        \n",
    "        pairs_at_confluence, basin_heads = first_meet_pairs_for_outlet(s, outlet)\n",
    "        \n",
    "        if not pairs_at_confluence:\n",
    "            print(\" (no pairs)\")\n",
    "            continue\n",
    "        \n",
    "        # Clear caches between outlets\n",
    "        coupling_an.clear_cache()\n",
    "        asymmetry_an.clear_cache()\n",
    "        \n",
    "        df_coupling = coupling_an.evaluate_pairs_for_outlet(outlet, pairs_at_confluence)\n",
    "        df_asymmetry = asymmetry_an.evaluate_pairs_for_outlet(outlet, pairs_at_confluence)\n",
    "        \n",
    "        n_pairs = len(df_coupling)\n",
    "        n_touching = df_coupling['touching'].sum() if not df_coupling.empty else 0\n",
    "        print(f\" ({n_pairs} pairs, {n_touching} touching)\")\n",
    "        \n",
    "        if not df_coupling.empty:\n",
    "            coupling_dfs.append(df_coupling)\n",
    "        if not df_asymmetry.empty:\n",
    "            asymmetry_dfs.append(df_asymmetry)\n",
    "    \n",
    "    if not coupling_dfs:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    df_coupling_all = pd.concat(coupling_dfs, ignore_index=True)\n",
    "    df_asymmetry_all = pd.concat(asymmetry_dfs, ignore_index=True)\n",
    "    df_combined = merge_coupling_and_asymmetry(df_coupling_all, df_asymmetry_all)\n",
    "    df_combined.sort_values([\"outlet\", \"confluence\", \"head_1\", \"head_2\"], inplace=True, ignore_index=True)\n",
    "    \n",
    "    return df_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Run Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results\n",
    "basin_results = {}\n",
    "basin_data = {}\n",
    "\n",
    "# Save experiment metadata\n",
    "metadata = {\n",
    "    \"experiment_name\": EXPERIMENT_NAME,\n",
    "    \"experiment_notes\": EXPERIMENT_NOTES,\n",
    "    \"stream_threshold\": STREAM_THRESHOLD,\n",
    "    \"connectivity\": CONNECTIVITY,\n",
    "    \"max_outlets_per_basin\": MAX_OUTLETS_PER_BASIN,\n",
    "    \"basins\": list(BASINS_TO_ANALYZE.keys()),\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(OUTPUT_DIR / \"experiment_metadata.json\", \"w\") as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(f\"Saved experiment metadata to: {OUTPUT_DIR / 'experiment_metadata.json'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "for basin_name, config in BASINS_TO_ANALYZE.items():\n",
    "    if not config[\"dem_path\"].exists():\n",
    "        print(f\"\\nSkipping {basin_name}: DEM not found\")\n",
    "        continue\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"Processing: {basin_name.upper()}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    basin_config = get_basin_config(config[\"paper_name\"])\n",
    "    z_th = basin_config[\"z_th\"]\n",
    "    lat = basin_config[\"lat\"]\n",
    "    \n",
    "    # Load and prepare\n",
    "    data = load_and_prepare_basin(\n",
    "        basin_name=basin_name,\n",
    "        dem_path=config[\"dem_path\"],\n",
    "        z_th=z_th,\n",
    "        stream_threshold=STREAM_THRESHOLD,\n",
    "    )\n",
    "    basin_data[basin_name] = data\n",
    "    \n",
    "    # Run analysis\n",
    "    print(f\"\\nRunning analysis (connectivity={CONNECTIVITY})...\")\n",
    "    df_results = run_full_analysis(\n",
    "        s=data[\"s\"],\n",
    "        fd=data[\"fd\"],\n",
    "        dem=data[\"dem\"],\n",
    "        lat=lat,\n",
    "        connectivity=CONNECTIVITY,\n",
    "        max_outlets=MAX_OUTLETS_PER_BASIN,\n",
    "    )\n",
    "    basin_results[basin_name] = df_results\n",
    "    \n",
    "    # Save per-basin results\n",
    "    output_path = OUTPUT_DIR / f\"{basin_name}_results.csv\"\n",
    "    df_results.to_csv(output_path, index=False)\n",
    "    print(f\"Saved: {output_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"Analysis complete! Results in: {OUTPUT_DIR}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "summary_rows = []\n",
    "\n",
    "for basin_name, df in basin_results.items():\n",
    "    if df.empty:\n",
    "        continue\n",
    "    \n",
    "    config = get_basin_config(BASINS_TO_ANALYZE[basin_name][\"paper_name\"])\n",
    "    ref = get_reference_delta_L(BASINS_TO_ANALYZE[basin_name][\"paper_name\"])\n",
    "    \n",
    "    n_pairs = len(df)\n",
    "    n_touching = df['touching'].sum()\n",
    "    touch_pct = 100 * n_touching / n_pairs if n_pairs > 0 else 0\n",
    "    delta_L_stats = compute_asymmetry_statistics(df['delta_L'].dropna())\n",
    "    \n",
    "    summary_rows.append({\n",
    "        \"Basin\": config[\"full_name\"].split(',')[0],\n",
    "        \"z_th (m)\": config[\"z_th\"],\n",
    "        \"Total Pairs\": n_pairs,\n",
    "        \"Touching\": n_touching,\n",
    "        \"Touch %\": f\"{touch_pct:.1f}%\",\n",
    "        \"delta_L (median)\": f\"{delta_L_stats['median']:.3f}\",\n",
    "        \"Reference delta_L\": f\"{ref['median']:.2f}\",\n",
    "    })\n",
    "\n",
    "df_summary = pd.DataFrame(summary_rows)\n",
    "print(f\"\\n=== Experiment: {EXPERIMENT_NAME} ===\")\n",
    "print(f\"Stream threshold: {STREAM_THRESHOLD}\")\n",
    "print()\n",
    "print(df_summary.to_string(index=False))\n",
    "\n",
    "# Save summary\n",
    "df_summary.to_csv(OUTPUT_DIR / \"summary.csv\", index=False)\n",
    "print(f\"\\nSaved summary to: {OUTPUT_DIR / 'summary.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plot-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Delta-L Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-delta-l",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot delta_L distribution for analyzed basins\n",
    "n_basins = len(basin_results)\n",
    "if n_basins == 0:\n",
    "    print(\"No results to plot\")\n",
    "else:\n",
    "    n_cols = min(4, n_basins)\n",
    "    n_rows = (n_basins + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(4*n_cols, 3*n_rows))\n",
    "    axes = np.atleast_1d(axes).flatten()\n",
    "    \n",
    "    for idx, (basin_name, df) in enumerate(basin_results.items()):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        if df.empty:\n",
    "            ax.set_title(f\"{basin_name}: No data\")\n",
    "            continue\n",
    "        \n",
    "        config = get_basin_config(BASINS_TO_ANALYZE[basin_name][\"paper_name\"])\n",
    "        ref = get_reference_delta_L(BASINS_TO_ANALYZE[basin_name][\"paper_name\"])\n",
    "        \n",
    "        delta_L = df['delta_L'].dropna()\n",
    "        stats = compute_asymmetry_statistics(delta_L)\n",
    "        \n",
    "        ax.hist(delta_L, bins=30, alpha=0.7, edgecolor='black')\n",
    "        ax.axvline(stats['median'], color='blue', linestyle='-', linewidth=2, \n",
    "                   label=f'Computed: {stats[\"median\"]:.2f}')\n",
    "        ax.axvline(ref['median'], color='red', linestyle='--', linewidth=2,\n",
    "                   label=f'Reference: {ref[\"median\"]:.2f}')\n",
    "        \n",
    "        ax.set_xlabel('delta_L')\n",
    "        ax.set_ylabel('Count')\n",
    "        short_name = config[\"full_name\"].split(',')[0]\n",
    "        ax.set_title(f\"{short_name}\\n(n={len(delta_L)}, th={STREAM_THRESHOLD})\")\n",
    "        ax.legend(fontsize=7)\n",
    "        ax.set_xlim(0, 2.5)\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for idx in range(len(basin_results), len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f'Experiment: {EXPERIMENT_NAME} (threshold={STREAM_THRESHOLD})', y=1.02, fontsize=14)\n",
    "    \n",
    "    # Save figure\n",
    "    fig.savefig(OUTPUT_DIR / \"delta_L_distribution.png\", dpi=150, bbox_inches='tight')\n",
    "    print(f\"Saved figure to: {OUTPUT_DIR / 'delta_L_distribution.png'}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Export Combined Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r9ydxjgsb6a",
   "source": "---\n## Aridity Index vs. Delta-L",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "9ghw66sf1ar",
   "source": "# Scatter plot: Aridity Index vs. Lengthwise Asymmetry (delta_L)\nif len(basin_results) == 0:\n    print(\"No results to plot\")\nelse:\n    fig, ax = plt.subplots(figsize=(12, 8))\n    \n    # Collect data for all analyzed basins\n    plot_data = []\n    for basin_name, df in basin_results.items():\n        if df.empty:\n            continue\n        \n        config = get_basin_config(BASINS_TO_ANALYZE[basin_name][\"paper_name\"])\n        ref = get_reference_delta_L(BASINS_TO_ANALYZE[basin_name][\"paper_name\"])\n        stats = compute_asymmetry_statistics(df['delta_L'].dropna())\n        \n        plot_data.append({\n            \"name\": config['full_name'].split(',')[0],\n            \"aridity\": config[\"aridity_index\"],\n            \"aridity_25\": config[\"aridity_index_25\"],\n            \"aridity_75\": config[\"aridity_index_75\"],\n            \"computed_median\": stats['median'],\n            \"computed_25\": stats['p25'],\n            \"computed_75\": stats['p75'],\n            \"ref_median\": ref['median'],\n            \"ref_25\": ref['p25'],\n            \"ref_75\": ref['p75'],\n        })\n    \n    # Plot computed values (blue circles)\n    for i, d in enumerate(plot_data):\n        ax.errorbar(\n            d[\"aridity\"], d[\"computed_median\"],\n            xerr=[[d[\"aridity\"] - d[\"aridity_25\"]], [d[\"aridity_75\"] - d[\"aridity\"]]],\n            yerr=[[d[\"computed_median\"] - d[\"computed_25\"]], [d[\"computed_75\"] - d[\"computed_median\"]]],\n            fmt='o', markersize=10, capsize=4, capthick=2,\n            color='dodgerblue', ecolor='dodgerblue', alpha=0.8,\n            label='Computed' if i == 0 else None\n        )\n    \n    # Plot reference values (red squares)\n    for i, d in enumerate(plot_data):\n        ax.errorbar(\n            d[\"aridity\"], d[\"ref_median\"],\n            xerr=[[d[\"aridity\"] - d[\"aridity_25\"]], [d[\"aridity_75\"] - d[\"aridity\"]]],\n            yerr=[[d[\"ref_median\"] - d[\"ref_25\"]], [d[\"ref_75\"] - d[\"ref_median\"]]],\n            fmt='s', markersize=8, capsize=4, capthick=2,\n            color='crimson', ecolor='crimson', alpha=0.8,\n            label='Paper Reference' if i == 0 else None\n        )\n    \n    # Connect computed and reference with dashed lines\n    for d in plot_data:\n        ax.plot([d[\"aridity\"], d[\"aridity\"]], \n                [d[\"computed_median\"], d[\"ref_median\"]], \n                'k--', alpha=0.3, linewidth=1)\n    \n    # Add labels\n    for d in plot_data:\n        y_pos = max(d[\"computed_median\"], d[\"ref_median\"])\n        ax.annotate(d[\"name\"], (d[\"aridity\"], y_pos),\n                    textcoords=\"offset points\", xytext=(5, 5), fontsize=8)\n    \n    ax.set_xlabel('Aridity Index', fontsize=12)\n    ax.set_ylabel('delta_L (Lengthwise Asymmetry)', fontsize=12)\n    ax.set_title(f'Aridity Index vs. delta_L - {EXPERIMENT_NAME} (th={STREAM_THRESHOLD})\\n'\n                 f'(Error bars show 25th-75th percentiles)', fontsize=12)\n    ax.legend(loc='upper left', fontsize=10)\n    ax.grid(True, alpha=0.3)\n    ax.set_xlim(0, None)\n    ax.set_ylim(0, None)\n    \n    plt.tight_layout()\n    fig.savefig(OUTPUT_DIR / \"aridity_vs_delta_L.png\", dpi=150, bbox_inches='tight')\n    print(f\"Saved figure to: {OUTPUT_DIR / 'aridity_vs_delta_L.png'}\")\n    plt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all results\n",
    "all_results = []\n",
    "for basin_name, df in basin_results.items():\n",
    "    if df.empty:\n",
    "        continue\n",
    "    df_copy = df.copy()\n",
    "    df_copy['basin'] = basin_name\n",
    "    config = get_basin_config(BASINS_TO_ANALYZE[basin_name][\"paper_name\"])\n",
    "    df_copy['basin_full_name'] = config['full_name']\n",
    "    df_copy['stream_threshold'] = STREAM_THRESHOLD\n",
    "    df_copy['experiment'] = EXPERIMENT_NAME\n",
    "    all_results.append(df_copy)\n",
    "\n",
    "if all_results:\n",
    "    df_all = pd.concat(all_results, ignore_index=True)\n",
    "    \n",
    "    combined_path = OUTPUT_DIR / \"all_basins_combined.csv\"\n",
    "    df_all.to_csv(combined_path, index=False)\n",
    "    \n",
    "    print(f\"\\nCombined results saved to: {combined_path}\")\n",
    "    print(f\"Total pairs: {len(df_all)}\")\n",
    "    print(f\"Total touching: {df_all['touching'].sum()}\")\n",
    "    print(f\"Basins: {df_all['basin'].nunique()}\")\n",
    "else:\n",
    "    print(\"No results to combine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment Complete\n",
    "\n",
    "Results have been saved to the experiment output directory. To compare with other experiments:\n",
    "\n",
    "```python\n",
    "# Load results from different experiments\n",
    "import pandas as pd\n",
    "from channel_heads import get_experiment_output_dir\n",
    "\n",
    "df_th145 = pd.read_csv(get_experiment_output_dir(\"th145_baseline\") / \"all_basins_combined.csv\")\n",
    "df_th500 = pd.read_csv(get_experiment_output_dir(\"th500_test\") / \"all_basins_combined.csv\")\n",
    "\n",
    "# Compare delta_L distributions\n",
    "print(f\"Threshold 145: median delta_L = {df_th145['delta_L'].median():.3f}\")\n",
    "print(f\"Threshold 500: median delta_L = {df_th500['delta_L'].median():.3f}\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}